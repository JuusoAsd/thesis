run_name: simple_inventory_pnl_reward_1684931295
eval_data:
  start_date: '2021_12_28'
  end_date: '2021_12_29'
train_data:
  start_date: '2021_12_30'
  end_date: '2022_01_03'
env:
  spaces:
    action_space: NormalizedAction
    observation_space:
      type: linear
      params: EverythingLinearSpace
  params:
    inv_envs: 4
    time_envs: 4
    data_portion: 0.5
    inv_jump: 0.18
  reward_space: simple_inventory_pnl
  reward_params:
    inventory_threshold: 0.5
    high_penalty: 0.5
venv:
  random: 1
clone: false
model:
  algo: PPO
  policy: MlpPolicy
  model_params:
    learning_rate: 0.0002682810046323
    n_steps: 32
    batch_size: 128
    n_epochs: 20
    gamma: 0.9999
    gae_lambda: 0.95
    clip_range: 0.2
    normalize_advantage: false
    ent_coef: 0.2
    vf_coef: 0.1549328961384911
    max_grad_norm: 5.0
    sde_sample_freq: 8
  policy_kwargs:
    net_arch:
      vf:
      - 256
      - 256
      pi:
      - 256
      - 256
expert_params:
  max_order_size: 5
  tick_size: 0.0001
  max_ticks: 10
  price_decimals: 4
  inventory_target: 0
  risk_aversion: 0.2
  order_size: 1
tuning:
  timesteps: 10000000
evaluation:
  callback:
    initial_expert: false
    save_best_model: false
    wait: 10
    freq: 3
    patience: 20
    improvement_thresh: 0.01
    time_envs: 6
    inv_envs: 1
    eval_mode: min_sharpe
