description: >
  Initial config for optimizing hparams
tags: [abs-param, ppo-lstm]
tune_log_path: hdd-tune-logs/thresh-1-10-ppo-no-lr-sched-lstm-jan25

objective:
  tune_train_start: "2021_12_24"
  tune_train_duration: 7
  validation_duration: 7
  timesteps: 10_000_000
  validation:
    wait: 10
    freq: 3
    patience: 3
    improvement_thresh: 0.01
    time_envs: 4
    inv_envs: 3

reporter:
  metric_columns: ["trial_reward", "group_reward", 'max_inventory','sharpe']
  sort_metric: "group_reward"
  report_frequency: 30
  max_progress_rows: 30

tuner:
  name: "tune_multiple_val_envs"
  samples: 2400

searcher:
  repeats: 8
  metric: "trial_reward"

initial_values:
  n_steps: 4
  batch_size: 2048
  gae_lambda: 0.95
  gamma: 0.999
  learning_rate: 1e-5
  max_grad_norm: 0.6
  vf_coef: 0.3
  ent_coef: 0.1
  clip_range: 0.5
  clip_range_vf: 0.2
  n_epochs: 10
  normalize_advantage: False
  # use_sde: False
  sde_sample_freq: 32
  # policy_kwargs.ortho_init: False
  # policy_kwargs.activation_fn: "nn.ReLU"
  # policy_kwargs.enable_critic_lstm: False
  # policy_kwargs.lstm_hidden_size: 256
  # policy_kwargs.net_arch: [512]

search_space:
  n_steps: ["choice", [4, 8, 16, 32]]
  batch_size: ["choice", [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]]
  gae_lambda: ["choice", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0]]
  gamma: ["choice", [0.995, 0.999, 0.9995, 0.9999]]
  learning_rate: ["loguniform", 1e-5, 0.001]
  max_grad_norm: ["choice", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5]]
  vf_coef: ["uniform", 0, 1]
  ent_coef: ["choice", [0, 1e-8, 1e-5, 0.01, 0.1, 0.2, 0.4, 0.5]]
  clip_range: ["choice", [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 0.9, 1.0]]
  clip_range_vf: ["choice", [null, 0.1, 0.2, 0.3, 0.5]] # TODO what values are good here?
  n_epochs: ["choice", [1, 5, 10, 20]]
  normalize_advantage: ["choice", [True, False]]
  sde_sample_freq: ["choice", [-1, 4, 8, 16, 32, 64, 128, 256]]
  # use_sde: ["choice", [True, False]] this does not work with loaded models?

  # policy_kwargs.ortho_init: ["choice", [True, False]]
  # policy_kwargs.activation_fn: ["choice", ["nn.ReLU", "nn.Tanh", "nn.LeakyReLU"]]
  # policy_kwargs.enable_critic_lstm: ["choice", [True, False]]
  # policy_kwargs.lstm_hidden_size: ["choice", [64, 128, 256, 512, 1024]]
  # policy_kwargs.net_arch:
  #   - choice
  #   - - [64]
  #     - [64, 64]
  #     - [512]
  #     - [512, 512]
  #     - [1024]
  #     - [2048]
  #     - [1024, 1024]
  #     - [2048, 2048]
  #     - [ 64, {"vf": [64], "pi": [64]}]
  #     - [ 512, {"vf": [512], "pi": [512]}]
  #     - [ 1024, {"vf": [1024], "pi": [1024]}]
  #     - [ 2048, {"vf": [128], "pi": [128]}]
  #     - [ 2048, {"vf": [256], "pi": [256]}]
  #     - [ 2048, {"vf": [512], "pi": [512]}]
  #     - [ 2048, {"vf": [2048], "pi": [2048]}]
  #     - [ 64, 64, {"vf": [64], "pi": [64]}]
  #     - [ 512, 512, {"vf": [512], "pi": [512]}]
  #     - [ 1024, 1024, {"vf": [1024], "pi": [1024]}]
  #     - [ 2048, 2048, {"vf": [2048], "pi": [2048]}]
  # constraints:
    # - "n_steps * n_envs % batch_size == 0"
    # - "(1 - use_sde) * sde_sample_freq in [0, -1]"
#      - "rl_check_freq >= n_steps * n_envs"
#      - "rl_check_freq % (n_steps * n_envs) == 0"

