
# ----------SETUP----------
run_name: preload_repeat
samples: 1000
trial_run: True

reporter: 
  parameter_columns: ["env.reward_space", "clone", "model.policy_kwargs.net_arch"]
  metric_columns:
    - "trial_group"
    - "trial_reward"
    - "group_reward"
    - "sharpe"
    - "returns"
    - "mean_abs_inv"
    - "max_inventory"
    - "duration"
  sort_metric: "group_reward"
  report_frequency: 60 # seconds
  report_rows: 30
  mode: min
searcher:
  repeats: 8
  metric: "group_reward" # what metric searcher optimizes for
  mode: min
  initial_values:
    env:
      spaces: 
        observation_space:
          params: SimpleLinearSpace
      reward: pnl

    clone: True
    model:
      # algo: PPO
      # policy: MlpPolicy
      model_params:
        learning_rate: 0.00035
        n_steps: 16
        batch_size: 512
        n_epochs: 5
        gamma: 0.999
        gae_lambda: 0.95
        clip_range: 0.3
        clip_range_vf: 0.2
        normalize_advantage: True
        ent_coef: 1e-5
        vf_coef: 0.65
        max_grad_norm: 0.5
        sde_sample_freq: 32


# ----------SEARCH----------
search_space:
  run_name: ${run_name}
  # ----------ENVIRONMENT----------
  eval_data:
    start_date: "2021_12_28"
    end_date: "2021_12_29"
  train_data:
    start_date: "2021_12_30"
    end_date: "2022_01_03"

  env: 
    spaces:
      action_space: NormalizedAction
      observation_space: 
        type: "linear"
        params: EverythingLinearSpace
    reward_space: 
      - choice
      - - assymetric_pnl_dampening
        - pnl
        - inventory
        - spreadpnl
        - multistep_pnl
        - inventory_integral_penalty
        - simple_inventory_pnl_reward
    params:
      inv_envs: 4
      time_envs: 4
      data_portion: 0.5
      inv_jump: 0.18  

  venv:
    random: 1

  # ----------MODEL----------
  clone: ["choice", [True, False]]
  model:
    algo: "PPO"
    model_name: clone_large
    policy: MlpPolicy
    model_params:
        learning_rate: ["loguniform", 1e-5, 0.001]
        n_steps: ["choice", [4, 8, 16, 32, 64, 128, 256, 512, 1024]]
        batch_size: ["choice", [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]]
        n_epochs: ["choice", [1, 5, 10, 20]]
        gamma: ["choice", [0.995, 0.999, 0.9995, 0.9999]]
        gae_lambda: ["choice", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0]]
        clip_range: ["choice", [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 0.9, 1.0]]
        clip_range_vf: ["choice", [null, 0.1, 0.2, 0.3, 0.5]]
        normalize_advantage: ["choice", [True, False]]
        ent_coef: ["choice", [0, 1e-8, 1e-5, 0.01, 0.1, 0.2, 0.4, 0.5]]
        vf_coef: ["uniform", 0, 1]
        max_grad_norm: ["choice", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5]]
        sde_sample_freq: ["choice", [-1, 4, 8, 16, 32, 64, 128, 256]]

    policy_kwargs: 
      net_arch: ["choice", [[32, 32], [64, 64], [128, 128], [256, 256], [512, 512], {"pi": [64,64] ,"vf": [64,64]}, {"vf": [256,256], "pi": [256,256]}]]

  # ----------EXPERT----------
  expert_params:
    max_order_size: 5
    tick_size: 0.0001
    max_ticks: 10
    price_decimals: 4
    inventory_target: 0
    risk_aversion: 0.2
    order_size: 1
      
  # ----------TUNING----------
  tuning:
    timesteps: 10_000_000
    callback:
      initial_expert: False
      save_best_model: False
      wait: 50
      freq: 5
      patience: 6
      improvement_thresh: 0.01
      time_envs: 1
      inv_envs: 1